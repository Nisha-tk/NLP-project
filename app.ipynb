{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKjS/TrneAat7BrRM08hqV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nisha-tk/NLP-project/blob/main/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q"
      ],
      "metadata": {
        "id": "y10mszlG-IQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d66c88-9732-40da-d742-bae6a5e272f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPQwRtKS_NaE",
        "outputId": "8e16c75e-2ae8-4af4-b7cf-174714f40b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcli9e9aH0K3",
        "outputId": "9c436bf9-670f-41a4-f207-51ce8f16c1f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmV2z92R-IGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "os.system('pkill -f streamlit')\n",
        "clear_output()\n"
      ],
      "metadata": {
        "id": "wFO-zALCTBFi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "tunnels = ngrok.get_tunnels()\n",
        "print(tunnels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr8tui0nTJXy",
        "outputId": "2525a4c1-b5e6-484f-d60e-ef1ddf70b3f8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<NgrokTunnel: \"https://9f9f-34-145-73-154.ngrok-free.app\" -> \"http://localhost:8501\">, <NgrokTunnel: \"https://7441-34-145-73-154.ngrok-free.app\" -> \"http://localhost:8501\">, <NgrokTunnel: \"https://22d8-34-145-73-154.ngrok-free.app\" -> \"http://localhost:8501\">]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tunnel in tunnels:\n",
        "    ngrok.disconnect(tunnel.public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfASX7xzTaOG",
        "outputId": "89f34539-7dc1-41db-f627-8ffdec69778a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-11-09T13:26:57+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-356505f0-8d80-49bf-8db1-8cd9ac73ffb3 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-11-09T13:26:58+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-f666d86a-1616-4927-b7b8-9ca7243863de acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-11-09T13:26:58+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-91abbd24-4fbe-44e6-88b5-d02c99d1fa9f acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyngrok streamlit pdfminer.six\n",
        "#!pip install pyngrok streamlit pdfminer.six\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Ensure no previous Streamlit instances are running\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run /content/app.py &>/dev/null&\n",
        "\n",
        "# Start ngrok to create a public URL\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Public URL: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjEhBmoFTeDu",
        "outputId": "d78fba3b-270f-4199-be11-1106a7b86229"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://4214-34-145-73-154.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'YOUR_AUTHTOKEN' with your actual ngrok authtoken\n",
        "!ngrok authtoken 2o72IKpaGtVYVAhMcgxKsPVgTKk_7b8zzVajuRrd6eaQQpi5r\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmqGvdDqxZJ4",
        "outputId": "aa71f901-f62d-453b-ab8a-143d78998eae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit\n"
      ],
      "metadata": {
        "id": "2LwjClXKuVjS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions_code = '''\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_contact_info(text):\n",
        "    contact_number = None\n",
        "    pattern = r\"\\\\b(?:\\\\+?\\\\d{1,3}[-.\\\\s]?)?\\\\(?\\\\d{3}\\\\)?[-.\\\\s]?\\\\d{3}[-.\\\\s]?\\\\d{4}\\\\b\"\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        contact_number = match.group()\n",
        "    return contact_number\n",
        "\n",
        "def extract_email_from_resume(text):\n",
        "    email = None\n",
        "    pattern = r\"\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\\\\b\"\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        email = match.group()\n",
        "    return email\n",
        "\n",
        "def extract_skills_from_resume(text, skills_list):\n",
        "    skills = []\n",
        "    for skill in skills_list:\n",
        "        pattern = r\"\\\\b{}\\\\b\".format(re.escape(skill))\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            skills.append(skill)\n",
        "    return skills\n",
        "\n",
        "def extract_education_from_resume(text):\n",
        "    education = []\n",
        "        education_keywords = [\n",
        "        'Computer Science', 'Information Technology', 'Software Engineering', 'Electrical Engineering', 'Mechanical Engineering', 'Civil Engineering',\n",
        "        'Chemical Engineering', 'Biomedical Engineering', 'Aerospace Engineering', 'Nuclear Engineering', 'Industrial Engineering', 'Systems Engineering',\n",
        "        'Environmental Engineering', 'Petroleum Engineering', 'Geological Engineering', 'Marine Engineering', 'Robotics Engineering', 'Biotechnology',\n",
        "        'Biochemistry', 'Microbiology', 'Genetics', 'Molecular Biology', 'Bioinformatics', 'Neuroscience', 'Biophysics', 'Biostatistics', 'Pharmacology',\n",
        "        'Physiology', 'Anatomy', 'Pathology', 'Immunology', 'Epidemiology', 'Public Health', 'Health Administration', 'Nursing', 'Medicine', 'Dentistry',\n",
        "        'Pharmacy', 'Veterinary Medicine', 'Medical Technology', 'Radiography', 'Physical Therapy', 'Occupational Therapy', 'Speech Therapy', 'Nutrition',\n",
        "        'Sports Science', 'Kinesiology', 'Exercise Physiology', 'Sports Medicine', 'Rehabilitation Science', 'Psychology', 'Counseling', 'Social Work',\n",
        "        'Sociology', 'Anthropology', 'Criminal Justice', 'Political Science', 'International Relations', 'Economics', 'Finance', 'Accounting', 'Business Administration',\n",
        "        'Management', 'Marketing', 'Entrepreneurship', 'Hospitality Management', 'Tourism Management', 'Supply Chain Management', 'Logistics Management',\n",
        "        'Operations Management', 'Human Resource Management', 'Organizational Behavior', 'Project Management', 'Quality Management', 'Risk Management',\n",
        "        'Strategic Management', 'Public Administration', 'Urban Planning', 'Architecture', 'Interior Design', 'Landscape Architecture', 'Fine Arts',\n",
        "        'Visual Arts', 'Graphic Design', 'Fashion Design', 'Industrial Design', 'Product Design', 'Animation', 'Film Studies', 'Media Studies',\n",
        "        'Communication Studies', 'Journalism', 'Broadcasting', 'Creative Writing', 'English Literature', 'Linguistics', 'Translation Studies',\n",
        "        'Foreign Languages', 'Modern Languages', 'Classical Studies', 'History', 'Archaeology', 'Philosophy', 'Theology', 'Religious Studies',\n",
        "        'Ethics', 'Education', 'Early Childhood Education', 'Elementary Education', 'Secondary Education', 'Special Education', 'Higher Education',\n",
        "        'Adult Education', 'Distance Education', 'Online Education', 'Instructional Design', 'Curriculum Development'\n",
        "        'Library Science', 'Information Science', 'Computer Engineering', 'Software Development', 'Cybersecurity', 'Information Security',\n",
        "        'Network Engineering', 'Data Science', 'Data Analytics', 'Business Analytics', 'Operations Research', 'Decision Sciences',\n",
        "        'Human-Computer Interaction', 'User Experience Design', 'User Interface Design', 'Digital Marketing', 'Content Strategy',\n",
        "        'Brand Management', 'Public Relations', 'Corporate Communications', 'Media Production', 'Digital Media', 'Web Development',\n",
        "        'Mobile App Development', 'Game Development', 'Virtual Reality', 'Augmented Reality', 'Blockchain Technology', 'Cryptocurrency',\n",
        "        'Digital Forensics', 'Forensic Science', 'Criminalistics', 'Crime Scene Investigation', 'Emergency Management', 'Fire Science',\n",
        "        'Environmental Science', 'Climate Science', 'Meteorology', 'Geography', 'Geomatics', 'Remote Sensing', 'Geoinformatics',\n",
        "        'Cartography', 'GIS (Geographic Information Systems)', 'Environmental Management', 'Sustainability Studies', 'Renewable Energy',\n",
        "        'Green Technology', 'Ecology', 'Conservation Biology', 'Wildlife Biology', 'Zoology']\n",
        "    for keyword in education_keywords:\n",
        "        pattern = r\"(?i)\\\\b{}\\\\b\".format(re.escape(keyword))\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            education.append(match.group())\n",
        "    return education\n",
        "\n",
        "def extract_name(text):\n",
        "    name = None\n",
        "    pattern = r\"(\\\\b[A-Z][a-z]+\\\\b)\\\\s(\\\\b[A-Z][a-z]+\\\\b)\"\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        name = match.group()\n",
        "    return name\n",
        "'''\n",
        "with open('resume.py', 'w') as f:\n",
        "    f.write(functions_code)\n"
      ],
      "metadata": {
        "id": "Y6BxETmDzEM1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "OgsV4BCl6XZz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_code = '''\n",
        "import streamlit as st\n",
        "import pickle\n",
        "from resume import extract_text_from_pdf, extract_contact_info, extract_email_from_resume, extract_skills_from_resume, extract_education_from_resume, extract_name\n",
        "import re\n",
        "\n",
        "# Function to clean the resume text\n",
        "def cleanResume(txt):\n",
        "    if txt is None:\n",
        "        return \"\"\n",
        "    cleanText = re.sub(r'http\\\\\\\\S+\\\\\\\\s', ' ', txt)\n",
        "    cleanText = re.sub(r'RT|cc', ' ', cleanText)\n",
        "    cleanText = re.sub(r'#\\\\\\\\S+\\\\\\\\s', ' ', cleanText)\n",
        "    cleanText = re.sub(r'@\\\\\\\\S+', '  ', cleanText)\n",
        "    cleanText = re.sub(r'[{}]'.format(re.escape(\"!\\\"#$%&'()*+,-./:;<=>?@[\\\\\\\\]^_`{|}~\")), ' ', cleanText)\n",
        "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText)\n",
        "    cleanText = re.sub(r'\\\\\\\\s+', ' ', cleanText)\n",
        "    return cleanText\n",
        "\n",
        "# Load the models\n",
        "try:\n",
        "    with open('/content/tfidf_vectorizer_job_recommendation.pkl', 'rb') as f:\n",
        "        tfidf_vectorizer = pickle.load(f)\n",
        "except (pickle.UnpicklingError, EOFError, AttributeError, ImportError, IndexError) as e:\n",
        "    st.error(\"Error loading tfidf_vectorizer: {}\".format(e))\n",
        "\n",
        "try:\n",
        "    with open('/content/rf_classifier_job_recommendation.pkl', 'rb') as f:\n",
        "        rf_classifier = pickle.load(f)\n",
        "except (pickle.UnpicklingError, EOFError, AttributeError, ImportError, IndexError) as e:\n",
        "    st.error(\"Error loading rf_classifier: {}\".format(e))\n",
        "\n",
        "# Define the list of skills directly (this should be consistent with the skills used in resume.py)\n",
        "skills_list = [\n",
        "    \"Python\", \"Java\", \"C++\", \"Machine Learning\", \"Data Analysis\", \"Project Management\",\n",
        "    \"Leadership\", \"SQL\", \"JavaScript\", \"HTML\", \"CSS\", \"Communication\", \"Problem-solving\"\n",
        "]\n",
        "\n",
        "st.title(\"Resume Parser and Job Recommendation System\")\n",
        "st.write(\"---\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload your resume (PDF)\", type=\"pdf\")\n",
        "\n",
        "if uploaded_file:\n",
        "    # Read and extract resume information\n",
        "    text = extract_text_from_pdf(uploaded_file)\n",
        "\n",
        "    # Debug: Check the extracted text\n",
        "    st.write(\"### Extracted Text:\")\n",
        "    st.write(text)\n",
        "\n",
        "    if text:\n",
        "        name = extract_name(text)\n",
        "        contact = extract_contact_info(text)\n",
        "        email = extract_email_from_resume(text)\n",
        "        skills = extract_skills_from_resume(text, skills_list)\n",
        "        education = extract_education_from_resume(text)\n",
        "\n",
        "        st.subheader(\"Extracted Information\")\n",
        "        st.write(\"**Name:** {}\".format(name))\n",
        "        st.write(\"**Contact:** {}\".format(contact))\n",
        "        st.write(\"**Email:** {}\".format(email))\n",
        "        st.write(\"**Skills:** {}\".format(\", \".join(skills)))\n",
        "        st.write(\"**Education:** {}\".format(education))\n",
        "\n",
        "        # Clean and vectorize the resume text\n",
        "        resume_text = cleanResume(text)\n",
        "\n",
        "        # Debug: Check the cleaned resume text\n",
        "        st.write(\"### Cleaned Resume Text:\")\n",
        "        st.write(resume_text)\n",
        "\n",
        "        if resume_text.strip():  # Ensure the cleaned text is not empty\n",
        "            try:\n",
        "                # Validate resume_text\n",
        "                if isinstance(resume_text, str):\n",
        "                    st.write(\"resume_text is a string\")\n",
        "                else:\n",
        "                    st.write(f\"resume_text is of type {type(resume_text)}\")\n",
        "\n",
        "                # Check the length of resume_text\n",
        "                st.write(f\"Length of resume_text: {len(resume_text)}\")\n",
        "\n",
        "                # Perform TF-IDF transformation\n",
        "                st.write(\"#### TF-IDF Transformation Status: Starting transformation\")\n",
        "                resume_tfidf = tfidf_vectorizer.transform([resume_text])\n",
        "                st.write(\"#### TF-IDF Transformation Status: Transformation successful\")\n",
        "\n",
        "                # Predict job recommendations\n",
        "                predicted_category = rf_classifier.predict(resume_tfidf)[0]\n",
        "\n",
        "                st.subheader(\"Job Recommendation\")\n",
        "                st.markdown(\"Based on your resume, we recommend the following job:\")\n",
        "                st.markdown(f\"### **{predicted_category}**\")\n",
        "            except Exception as e:\n",
        "                st.error(\"Error during TF-IDF transformation or prediction: {}\".format(e))\n",
        "        else:\n",
        "            st.error(\"The cleaned resume text is empty. Please check the content of your resume.\")\n",
        "    else:\n",
        "        st.error(\"Could not extract text from the uploaded PDF. Please ensure it is a valid PDF file.\")\n",
        "'''\n",
        "\n",
        "with open('/content/app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_code)\n"
      ],
      "metadata": {
        "id": "RQA1qpjZAtP9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJk4mprgGW3Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}